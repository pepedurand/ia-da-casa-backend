import {
  BadRequestException,
  Injectable,
  InternalServerErrorException,
} from '@nestjs/common';
import { ChatBody, ChatResponse, OpenAIModel } from './dto/chat.dto';
import OpenAI from 'openai';
import { CreateAgentDto, CreateAgentResponseDto } from './dto/create-agent.dto';
import { InjectRepository } from '@nestjs/typeorm';
import { ConversasAgentes } from './conversas-agentes.entity';
import { Repository } from 'typeorm';
import { Agentes } from './agentes.entity';

const POLLING_CONFIG = {
  MAX_ATTEMPTS: 15,
  INITIAL_DELAY: 1000,
  MAX_DELAY: 8000,
  BACKOFF_MULTIPLIER: 1.5,
};

@Injectable()
export class IaService {
  private openai: OpenAI;

  constructor(
    @InjectRepository(ConversasAgentes)
    private conversasRepository: Repository<ConversasAgentes>,
    @InjectRepository(Agentes)
    private agentesRepository: Repository<Agentes>,
  ) {
    this.openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });
  }

  async chat(chatData: ChatBody, userId: string): Promise<any> {
    const {
      agentId,
      prompt,
      model = OpenAIModel.GPT_4O,
      threadId: thread_id,
      contextId,
    } = chatData;

    const tools = [
      {
        type: 'function' as const,
        name: 'get_horoscope',
        description: "Get today's horoscope for an astrological sign.",
        parameters: {
          type: 'object',
          properties: {
            sign: {
              type: 'string',
              description: 'An astrological sign like Taurus or Aquarius',
            },
          },
          required: ['sign'],
        },
        strict: true,
      },
    ];

    // Create a running input list we will add to over time
    let input: any[] = [
      {
        role: 'user' as const,
        content: prompt,
      },
    ];

    // 2. Prompt the model with tools defined
    let response = await this.openai.responses.create({
      model: 'gpt-5',
      tools,
      input,
    });

    // Save function call outputs for subsequent requests
    let functionCall: (typeof response.output)[number] | undefined;
    let functionCallArguments: any;
    input.push(...response.output);

    response.output.forEach((item) => {
      if (item.type === 'function_call') {
        functionCall = item;
        functionCallArguments = JSON.parse(item.arguments);
      }
    });

    // 3. Execute the function logic for get_horoscope
    function getHoroscope(sign: string) {
      return sign + ' Next Tuesday you will befriend a baby otter.';
    }
    const result = { horoscope: getHoroscope(functionCallArguments?.sign) };

    // 4. Provide function call results to the model
    if (!functionCall || functionCall.type !== 'function_call') {
      throw new InternalServerErrorException('Function call is undefined.');
    }
    input.push({
      type: 'function_call_output',
      // ⬇️ USE O call_id DO ITEM function_call
      call_id: functionCall.call_id, // <-- ajuste aqui
      output: JSON.stringify(result),
    });

    response = await this.openai.responses.create({
      model: 'gpt-5',
      instructions: 'Respond only with a horoscope generated by a tool.',
      tools,
      input,
      // parallel_tool_calls: false, // (opcional) se quiser garantir 1 por vez
    });

    console.log('Final output:');
    console.log(JSON.stringify(response.output, null, 2));

    // console.log(`Iniciando chat com agente ${agentId} para usuário ${userId}`);

    // try {
    //   let threadRecord: ConversasAgentes | null;
    //   let threadId: string;

    //   if (thread_id) {
    //     threadRecord = await this.conversasRepository.findOne({
    //       where: { userId, threadId: thread_id },
    //     });

    //     if (!threadRecord) {
    //       console.error(
    //         `Thread ${thread_id} não encontrada ou não pertence ao usuário ${userId}`,
    //       );
    //       throw new BadRequestException(
    //         'Conversa não encontrada ou não pertence ao usuário',
    //       );
    //     }

    //     threadId = threadRecord.threadId;
    //     threadRecord.lastUpdated = new Date();
    //     await this.conversasRepository.save(threadRecord);

    //     console.log(
    //       `Usando thread existente ${threadId} para usuário ${userId} e agente ${agentId}`,
    //     );
    //   } else {
    //     const newThread: any = await this.openai.request({
    //       method: 'post',
    //       path: '/threads',
    //       headers: { 'OpenAI-Beta': 'assistants=v2' },
    //     });
    //     threadId = newThread.id;

    //     const title = await this.generateTitle(prompt);

    //     console.log(
    //       `Criado novo thread ${threadId} para usuário ${userId} e agente ${agentId} com título: ${title}`,
    //     );

    //     threadRecord = this.conversasRepository.create({
    //       userId,
    //       agent: agentId,
    //       threadId,
    //       title,
    //     });

    //     await this.conversasRepository.save(threadRecord);
    //     // Se usar cache, descomente:
    //     // const conversationsCacheKey = `openai-agents/conversations/${userId}`;
    //     // await this.cacheManager.del(conversationsCacheKey);
    //   }

    //   await this.openai.request({
    //     method: 'post',
    //     path: `/threads/${threadId}/messages`,
    //     body: { role: 'user', content: prompt },
    //     headers: { 'OpenAI-Beta': 'assistants=v2' },
    //   });

    //   const run: any = await this.openai.request({
    //     method: 'post',
    //     path: `/threads/${threadId}/runs`,
    //     body: {
    //       assistant_id: agentId,
    //       model,
    //     },
    //     headers: { 'OpenAI-Beta': 'assistants=v2' },
    //   });

    //   await this.waitForRunCompletion(threadId, run.id);

    //   const reply = (await this.fetchLastAssistantMessage(threadId)) ?? '';

    //   return {
    //     reply,
    //     runId: run.id,
    //     threadId,
    //   };
    // } catch (error: any) {
    //   console.error(
    //     `Erro ao processar chat com agente: ${error.message}`,
    //     error.stack,
    //   );
    //   if (error instanceof BadRequestException) {
    //     throw error;
    //   }
    //   throw new InternalServerErrorException(
    //     `Erro ao processar chat com agente: ${error.message}`,
    //   );
    // }
  }

  async createAgent(
    userId: string,
    agentData: CreateAgentDto,
  ): Promise<CreateAgentResponseDto> {
    try {
      console.log(
        `Criando novo agente: ${agentData.name} para usuário ${userId}`,
      );

      const tools =
        agentData.tools?.map((tool) => {
          if (tool === 'code_interpreter') {
            return { type: 'code_interpreter' as const };
          } else if (tool === 'file_search') {
            return { type: 'file_search' as const };
          }
          return { type: tool as any };
        }) || [];

      const assistant = await this.openai.beta.assistants.create({
        name: agentData.name,
        instructions: agentData.instructions,
        description: agentData.description,
        model: agentData.model ?? OpenAIModel.GPT_4O,
        tools,
        temperature: agentData.temperature,
        top_p: agentData.top_p,
      });

      const agentRecord = this.agentesRepository.create({
        userId,
        agentId: assistant.id,
      });
      await this.agentesRepository.save(agentRecord);

      // const cacheKey = `openai-agents/agents/${userId}`;
      // await this.cacheManager.del(cacheKey);

      console.log(`Agente criado com sucesso: ${assistant.id}`);

      return {
        id: assistant.id,
        name: assistant.name || '',
        description: assistant.description || '',
        instructions: assistant.instructions || '',
        model: assistant.model,
        created_at: new Date(assistant.created_at * 1000),
      };
    } catch (error) {
      console.error(`Erro ao criar agente: ${error.message}`, error.stack);
      throw new InternalServerErrorException(
        `Erro ao criar agente: ${error.message}`,
      );
    }
  }

  private async fetchLastAssistantMessage(
    threadId: string,
  ): Promise<string | null> {
    const messages: any = await this.openai.request({
      method: 'get',
      path: `/threads/${threadId}/messages`,
      query: { limit: 30, order: 'desc' },
      headers: { 'OpenAI-Beta': 'assistants=v2' },
    });

    const lastMessage = messages.data?.[0];
    if (!lastMessage || lastMessage.role !== 'assistant') return null;

    let text = '';
    for (const content of lastMessage.content ?? []) {
      if (content.type === 'text') text += content.text?.value ?? '';
    }
    return text || null;
  }

  private async waitForRunCompletion(
    threadId: string,
    runId: string,
  ): Promise<void> {
    let delay = POLLING_CONFIG.INITIAL_DELAY;

    for (let attempt = 0; attempt < POLLING_CONFIG.MAX_ATTEMPTS; attempt++) {
      const run: any = await this.openai.request({
        method: 'get',
        path: `/threads/${threadId}/runs/${runId}`,
        headers: { 'OpenAI-Beta': 'assistants=v2' },
      });

      const status = run.status as string;

      if (status === 'completed') return;

      if (status === 'requires_action') {
        await this.handleRequiredAction(threadId, runId, run);
      } else if (
        status === 'failed' ||
        status === 'cancelled' ||
        status === 'expired'
      ) {
        throw new InternalServerErrorException(
          `Execução falhou com status: ${status}`,
        );
      }

      await this.sleep(delay);
      delay = Math.min(
        delay * POLLING_CONFIG.BACKOFF_MULTIPLIER,
        POLLING_CONFIG.MAX_DELAY,
      );
    }

    throw new InternalServerErrorException(
      `Timeout aguardando conclusão da execução (máximo ${POLLING_CONFIG.MAX_ATTEMPTS} tentativas)`,
    );
  }

  private async handleRequiredAction(
    threadId: string,
    runId: string,
    run: any,
  ) {
    const calls = run?.required_action?.submit_tool_outputs?.tool_calls ?? [];
    if (!calls.length) return;

    const outputs: { tool_call_id: string; output: string }[] = [];

    for (const call of calls) {
      const toolName = call.function?.name;
      const argsStr = call.function?.arguments ?? '{}';

      let result: any;
      try {
        const args = JSON.parse(argsStr);
        // TODO: integre seu ToolRegistry real aqui:
        // result = await this.toolRegistry.execute(toolName, args);
        result = { ok: true, echo: { toolName, args } }; // mock
      } catch (e: any) {
        result = { ok: false, error: e?.message };
      }

      outputs.push({
        tool_call_id: call.id,
        output: JSON.stringify(result),
      });
    }

    await this.openai.request({
      method: 'post',
      path: `/threads/${threadId}/runs/${runId}/submit_tool_outputs`,
      body: { tool_outputs: outputs },
      headers: { 'OpenAI-Beta': 'assistants=v2' },
    });
  }

  private async generateTitle(prompt: string): Promise<string> {
    try {
      const completion = await this.openai.chat.completions.create({
        model: OpenAIModel.GPT_4O,
        messages: [
          {
            role: 'system',
            content:
              'Gere um título conciso e descritivo (máximo 50 caracteres) para a seguinte conversa. Responda apenas com o título, sem aspas ou formatação.',
          },
          { role: 'user', content: prompt },
        ],
        max_tokens: 20,
        temperature: 0.7,
      });

      return (
        completion.choices[0]?.message?.content?.trim() || 'Conversa sem título'
      );
    } catch {
      return 'Conversa sem título';
    }
  }

  private sleep(ms: number) {
    return new Promise((resolve) => setTimeout(resolve, ms));
  }
}
